{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LucjanJanowski/Generalized-Score-Distribution/blob/master/Generalized_Score_Distribution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ielOh8Rj_iSG"
   },
   "source": [
    "##### Copyright 2019 Krzystzof Rusek,\n",
    "\n",
    "AGH University of Science and Technology\n",
    "Licensed under the MIT License (the \\\"License\\\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DQbGbWrAGGb"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the MIT License (the \"License\"); { display-mode:\"form\" }\n",
    "# MIT License\n",
    "\n",
    "# Copyright (c) 2018 Krzystzof Rusek and Lucjan Janowski\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qecYN8GBFZHv"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/LucjanJanowski/Generalized-Score-Distribution/blob/master/Generalized_Score_Distribution.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/LucjanJanowski/Generalized-Score-Distribution/blob/master/Generalized_Score_Distribution.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "koLlhIRu1erI"
   },
   "source": [
    "# Generalized Score Distribution\n",
    "\n",
    "###### By Bogdan Ćmiel\n",
    "\n",
    "Generalized Score Distributon (GSD) describes a class of two parameters distributions definded on discete support $\\{1, 2, \\cdots, M\\}$ with no more than one change in the probability monotonicity. It means that for increasing index of the probability we can have:\n",
    "* not increasing probabilities \n",
    "* not increasing to the index $k$ and than not decreasing\n",
    "* not decreasing to the index $k$ and than not increasing \n",
    "* not decreasing probabilities\n",
    "\n",
    "The above equations define GSD. \n",
    "\n",
    "Let us denote \n",
    "$$V_{\\mathrm{min}}(\\psi)=(\\lceil\\psi\\rceil-\\psi)(\\psi-\\lfloor\\psi\\rfloor),$$\n",
    "$$V_{\\mathrm{max}}(\\psi)=(\\psi-1)(M-\\psi),$$\n",
    "$$C(\\psi)=\\frac{M-2}{M-1}\\ \\frac{V_{\\mathrm{max}}(\\psi)}{V_{\\mathrm{max}}(\\psi)-V_{\\mathrm{min}}(\\psi)}.$$\n",
    "Let\n",
    "\\begin{equation} \n",
    "\t\\begin{split}\n",
    "\t\tP_{F_\\rho} & (\\epsilon=k-\\psi)=  \n",
    "\t\t \\frac{\\rho-C(\\psi)}{1-C(\\psi)}[1-|k-\\psi|]_{+} + \\frac{1-\\rho}{1-C(\\psi)} \n",
    "\t\t \\binom{M-1}{k-1}\\left(\\frac{\\psi-1}{M-1}\\right)^{k-1}\\left(\\frac{M-\\psi}{M-1}\\right)^{M-k},\n",
    "\t\\end{split}\n",
    "\\end{equation}\n",
    "where $\\rho\\in[C(\\psi),1]$, $k=1,...,M$ and\n",
    "\\begin{equation}\n",
    "\t\\begin{split}\n",
    "\tP_{G_{\\rho}}& (\\epsilon=k-\\psi)\t= \\binom{M-1}{k-1}\\\\\n",
    "\t& \\frac{\\mathcal{B}\\left(\\frac{(\\psi-1)\\rho}{(M-1)(C(\\psi)-\\rho)}+k-1,\\frac{(M-\\psi)\\rho}{(M-1)(C(\\psi)-\\rho)}+M-k\\right)}{\\mathcal{B}\\left(\\frac{(\\psi-1)\\rho}{(M-1)(C(\\psi)-\\rho)},\\frac{(M-\\psi)\\rho}{(M-1)(C(\\psi)-\\rho)}\\right)}, \n",
    "\t\\end{split}\n",
    "\\end{equation}\n",
    "where $\\rho\\in(0,C(\\psi))$, $k=1,...,M$.\n",
    "\n",
    "If we denote by $H_{\\rho}$ the distribution function of the noise then we will assume that\n",
    "\n",
    "$$ H_{\\rho}=G_\\rho\\ I(\\rho<C(\\psi)) + F_\\rho\\ I(\\rho\\geq C(\\psi))$$\n",
    "\n",
    "where $\\rho\\in(0,1]$ is a confidence parameter (see Remark ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WiQNFIqtAfHX"
   },
   "source": [
    "\n",
    "### Remark\n",
    "\n",
    "The variance of the noise is equal to\n",
    "$$\\mathbb{V}_{H_{\\rho}}(\\epsilon)=\\rho V_{\\mathrm{min}}(\\psi)+(1-\\rho)V_{\\mathrm{max}}(\\psi).$$ \n",
    "\n",
    "Since the variance of the noise is a decreasing function of $\\rho\\in(0,1]$ then this parameter has an interpretation as a confidence parameter. \n",
    "\n",
    "\n",
    "From this moment we will assume the following\n",
    " \n",
    "$$U=\\psi+\\epsilon,$$ \n",
    "where $\\psi\\in[1,M]$ is an unknown parameter, $\\epsilon$ are independent random variables with distribution functions $H_{\\rho}$, where $\\rho\\in(0,1]$ is an unknown parameter.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YD7NfXOA7SL1"
   },
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "k17TRXyC36JR"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8aPVHTx4N15"
   },
   "outputs": [],
   "source": [
    "#@title Functions { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "#@markdown Definition of `log_prob` and `sample` functions\n",
    "\n",
    "def log_newton(n,k):\n",
    "    \"\"\"\n",
    "    Logarithm of Binomial symbol - made to be numerically stable\n",
    "    \"\"\"\n",
    "    return tf.math.lgamma(n+1) -\\\n",
    "          tf.math.lgamma(k+1) -\\\n",
    "          tf.math.lgamma(n-k+1)\n",
    "\n",
    "def log_beta(x,y):\n",
    "    return tf.math.lgamma(x) + tf.math.lgamma(y) - tf.math.lgamma(x+y) \n",
    "\n",
    "def log_prob(psi, rho, u, M, dtype=tf.float64):\n",
    "    \"\"\"\n",
    "    log probability of Generalized Score Distribution\n",
    "\n",
    "    :param psi: psi parameters\n",
    "    :param rho: $rho in (0,1)$ parameters\n",
    "    :param u: the categories for which we are looking for the probabilities (1, 2, 3, 4,...M)\n",
    "    :param M: a value corresponding to the top category\n",
    "    :param dtype: a data type to use for computations\n",
    "\n",
    "    :return: log of the given probability\n",
    "    \"\"\"\n",
    "    \n",
    "    c1 = tf.constant(1.0, dtype=dtype)\n",
    "    c2 = tf.constant(2.0, dtype=dtype)\n",
    "\n",
    "  \n",
    "    Vmin = (tf.math.ceil(psi)-psi)*(psi-tf.math.floor(psi))\n",
    "    Vmax = (psi-c1)*(M-psi)\n",
    "    C = (M-c2)/(M-c1)*Vmax/(Vmax - Vmin)\n",
    "    \n",
    "    log_beta_bin = log_newton(M-c1,u-c1) + \\\n",
    "        log_beta(rho*(psi-c1)/( (M-c1) *(C-rho))+u -c1, (M-psi)*rho/( (M-c1) *(C-rho)) + M -u) - \\\n",
    "        log_beta(rho*(psi-c1)/( (M-c1) *(C-rho)), (M-psi)*rho/( (M-c1) *(C-rho)))\n",
    "    \n",
    "    log_binomial =  log_newton(M-c1,u-c1) + \\\n",
    "        (u-c1)*(tf.math.log(psi-c1) - tf.math.log(M-c1) ) + \\\n",
    "        (M -u)*(tf.math.log(M-psi) - tf.math.log(M-c1) )\n",
    "    \n",
    "    dist_binomial = tf.exp(log_newton(M-c1,u-c1))*tf.pow(psi-c1,u-c1)*tf.pow(M-psi,M-u)/tf.pow(M-c1,M-c1)\n",
    "    dist_min = tf.nn.relu(c1 - tf.abs(u-psi))\n",
    "    \n",
    "    ratio = (rho -C)/(c1-rho)*dist_min/dist_binomial\n",
    "    \n",
    "    logmix = log_binomial + tf.math.log1p(-rho) - tf.math.log1p(-C) + \\\n",
    "        tf.math.log1p(ratio)\n",
    "    \n",
    "    condition = tf.broadcast_to(rho,shape=u.shape) < C\n",
    "    \n",
    "    lprob = tf.where(condition,log_beta_bin, logmix)\n",
    "    return lprob\n",
    "\n",
    "def convert_and_brodcast(n, shape, dtype):\n",
    "    '''Broadcast with type convertion\n",
    "    Args:\n",
    "        n (Tensor)\n",
    "        shape (tensor_shape)\n",
    "        \n",
    "    Returns:\n",
    "        Converted and broadcasted n\n",
    "    '''\n",
    "    tn = tf.convert_to_tensor(n,dtype=dtype)\n",
    "    brodcasted = tf.broadcast_to(tn,shape)\n",
    "    return brodcasted\n",
    "\n",
    "def sample(psi,rho,M,num_samples=1, dtype=tf.float64):\n",
    "    '''\n",
    "    Sample the Generalized Score Distribution. \n",
    "    Args:\n",
    "        psi - psi parameters \n",
    "        rho - rho parameters\n",
    "        num_samples - number of samples for specific psi and rho pair\n",
    "    Returns:\n",
    "        tensor with 3 dimensions if num_samples > 1, we should use more than one psi and rho\n",
    "    '''\n",
    "\n",
    "    support=tf.linspace(1.,M,tf.cast(M,tf.int32))\n",
    "    support = tf.cast(support, dtype)\n",
    "\n",
    "    lprobs = log_prob(psi,rho,support,M)\n",
    "    lprobs = tf.expand_dims(lprobs, axis=0)\n",
    "    samples = 1+tf.random.categorical(lprobs, num_samples=num_samples)\n",
    "    return samples\n",
    "\n",
    "def chi_squared_test(obs_scores, psi, rho, max_score):\n",
    "    \"\"\"\n",
    "    Runs the chi-squared goodness-of-fit test. It does so for a single vector of individual scores for a single test\n",
    "    material. Actually observed scores (obs_scores) are compared with the expected ones. The expected ones are generated\n",
    "    using the GSD model, given psi (psi) and rho (rho) parameters.\n",
    "\n",
    "    Returns a single p-value related with a null hypothesis that the GSD model accurately describes the data observed\n",
    "    (for a single test material being tested).\n",
    "\n",
    "    WARNING: This function assumes that the scale always starts at 1 and increments by 1, up to max_score.\n",
    "\n",
    "    :param obs_scores: actually observed individual scores for a given test material (a 1D vector)\n",
    "    :param psi: the psi parameter estimated from the actually observed individual scores\n",
    "    :param rho: the rho parameter estimated from the actually observed individual scores\n",
    "    :param max_score: a value corresponding to the top category of the scale (e.g. for 5-levels ACR this equals 5)\n",
    "    :return: a single p-value related with a null hypothesis that the GSD model accurately describes the data observed\n",
    "        (for a single test material being tested)\n",
    "    \"\"\"\n",
    "    g = tf.Graph()\n",
    "\n",
    "    n_total_scores = len(obs_scores)\n",
    "\n",
    "    with g.as_default():\n",
    "        # Generate expected probabilities of each score occurring\n",
    "\n",
    "        support = tf.linspace(1., np.float64(max_score), int(max_score))\n",
    "        support = tf.cast(support, tf.float64)\n",
    "\n",
    "        log_probs = log_prob(np.float64(psi), np.float64(rho), support, np.float64(max_score))\n",
    "\n",
    "    with tf.compat.v1.Session(graph=g) as sess:\n",
    "        # Read the log probability of each score\n",
    "        # (np appendix means a variable is a numpy array)\n",
    "        cat_log_probs, support_np = sess.run([log_probs, support])\n",
    "\n",
    "        # Translate those into probabilities\n",
    "        cat_probs = np.exp(cat_log_probs)\n",
    "\n",
    "        # Generate how many ratings we expect to observe for each score\n",
    "        expected_counts = cat_probs*n_total_scores\n",
    "\n",
    "        # Translate observed individual ratings into counts of each score\n",
    "        observed_counts = np.sum(np.equal(obs_scores, support_np[:, np.newaxis]) , axis=1)\n",
    "\n",
    "        # Run chi-squared test\n",
    "        chi_sq, p_val = sts.chisquare(f_obs = observed_counts, f_exp = expected_counts, ddof=int(max_score-1-2))\n",
    "\n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQBKp71f7a3b"
   },
   "source": [
    "# Fit\n",
    "\n",
    "* Estimate parameters for your own data \n",
    "* Test estimation accuracy \n",
    "* Plot the probability mass function\n",
    "\n",
    "***Double click the form title to check the implementation details***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "gUjoqp6N2F1t",
    "outputId": "1d0625d9-3542-4209-d549-a0e55891b930"
   },
   "outputs": [],
   "source": [
    "#@title User data {vertical-output: true, display-mode: \"form\" }\n",
    "\n",
    "max_score=5 #@param {type:\"integer\"}\n",
    "max_step=5000 #@param {type:\"integer\"}\n",
    "grad_tol=1e-8 #@param {type:\"number\"}\n",
    "#@markdown Enter coma separated scores\n",
    "scores='5,5,5,4,3,3' #@param {type:\"string\"}\n",
    "\n",
    "\n",
    "scores = [int(i) for i in scores.split(',')]\n",
    "\n",
    "\n",
    "g=tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "\n",
    "    samples = tf.convert_to_tensor(scores)\n",
    "    u = tf.cast(samples, tf.float64)\n",
    "    M=tf.convert_to_tensor(max_score,dtype=tf.float64)\n",
    "\n",
    "    varrho = tf.Variable(tf.random_normal([],dtype=tf.float64,stddev=1.1))\n",
    "    varpsi = tf.Variable(tf.random_normal([],dtype=tf.float64,stddev=0.1))\n",
    "\n",
    "    bottom = tf.constant(1.0, dtype=tf.float64)\n",
    "    top = tf.convert_to_tensor(M-1, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    rho = tf.nn.sigmoid(varrho) # [0,1]\n",
    "    psi = bottom + top * tf.nn.sigmoid(varpsi) # [1,M]\n",
    "\n",
    "\n",
    "    loglik = log_prob(psi,rho,u,M)\n",
    "    loss = -tf.reduce_mean(loglik)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
    "\n",
    "    v = [varpsi, varrho]\n",
    "    grad = tf.gradients(loss,v)\n",
    "    train_op = optimizer.apply_gradients(zip(grad,v))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as ses:\n",
    "    ses.run(tf.global_variables_initializer())\n",
    "    for i in range(max_step):\n",
    "        _,l,pygrad,psihat,rhohat = ses.run([train_op,loss,grad,psi,rho])\n",
    "        if max(map(np.linalg.norm, pygrad)) < grad_tol:\n",
    "            break\n",
    "        if i % 100 ==0:\n",
    "          print('#',end='')\n",
    "    psi_hat, rho_hat = ses.run([psi,rho])\n",
    "\n",
    "print('')\n",
    "print('Ψ={0:.3f}, ρ={1:.3f}'.format( psi_hat, rho_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "uYC-0FJmHLll",
    "outputId": "70a65922-0307-44b8-c158-15035e9cb01e"
   },
   "outputs": [],
   "source": [
    "#@title  Fitting from random sample { vertical-output: true, display-mode: \"form\" }\n",
    "\n",
    "max_score=8 #@param {type:\"integer\"}\n",
    "max_step=5000 #@param {type:\"integer\"}\n",
    "grad_tol=1e-8 #@param {type:\"number\"}\n",
    "\n",
    "true_psi=5.3 #@param {type:\"number\"}\n",
    "\n",
    "true_rho=0.5 #@param {type:\"number\"}\n",
    "\n",
    "n_samples=80 #@param {type:\"integer\"}\n",
    "\n",
    "g=tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "\n",
    "    samples= sample(np.float64(true_psi),np.float64(true_rho),np.float64(max_score),n_samples)\n",
    "\n",
    "\n",
    "    u = tf.cast(samples, tf.float64)\n",
    "    M=tf.convert_to_tensor(max_score,dtype=tf.float64)\n",
    "\n",
    "    varrho = tf.Variable(tf.random_normal([],dtype=tf.float64,stddev=1.1))\n",
    "    varpsi = tf.Variable(tf.random_normal([],dtype=tf.float64,stddev=0.1))\n",
    "\n",
    "    c1 = tf.constant(1.0, dtype=tf.float64)\n",
    "    top = tf.convert_to_tensor(M-1, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    rho = tf.nn.sigmoid(varrho) # [0,1]\n",
    "    psi = c1 + top * tf.nn.sigmoid(varpsi) # [1,M]\n",
    "\n",
    "\n",
    "    ll = log_prob(psi,rho,u,M)\n",
    "    loss = -tf.reduce_mean(ll)\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n",
    "\n",
    "    v = [varpsi, varrho]\n",
    "    grad = tf.gradients(loss,v)\n",
    "    train_op = optimizer.apply_gradients(zip(grad,v))\n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as ses:\n",
    "    ses.run(tf.global_variables_initializer())\n",
    "    for i in range(max_step):\n",
    "        _,l,pygrad,psihat,rhohat = ses.run([train_op,loss,grad,psi,rho])\n",
    "        if max(map(np.linalg.norm, pygrad)) < grad_tol:\n",
    "            break\n",
    "        if i % 100 ==0:\n",
    "          print('#',end='')\n",
    "    psi_hat, rho_hat = ses.run([psi,rho])\n",
    "\n",
    "print('\\nΨ={0:.3f}, ρ={1:.3f}'.format( psi_hat, rho_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 271
    },
    "colab_type": "code",
    "id": "G8xU6OjQM4o7",
    "outputId": "ecdbf267-6812-4c64-811b-99d7a1f96470"
   },
   "outputs": [],
   "source": [
    "#@title Distribution { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "\n",
    "max_score=8 #@param {type:\"integer\"}\n",
    "psi=6.5 #@param {type:\"number\", min:1.0}\n",
    "rho = 0.9 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n",
    "\n",
    "\n",
    "\n",
    "g=tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "\n",
    "    support=tf.linspace(1.,max_score,max_score)\n",
    "    support = tf.cast(support, tf.float64)\n",
    "\n",
    "    lprobs = log_prob(np.float64(psi),\n",
    "                      np.float64(rho),\n",
    "                      support,\n",
    "                      np.float64(max_score))\n",
    "\n",
    "    probs = tf.exp(lprobs)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "with tf.Session(graph=g) as ses:\n",
    "    pyprobs,pysupport=ses.run([probs,support])\n",
    "    #print(pyprobs);\n",
    "\n",
    "\n",
    "ax=plt.subplot();\n",
    "\n",
    "ax.stem(pysupport, pyprobs);\n",
    "ax.axvline(psi, color='black');\n",
    "\n",
    "ax.legend(['mean','distribution']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\chi^2$ goodness-of-fit test\n",
    "\n",
    "Having estimated $\\psi$ and $\\rho$ for a given test material, we can check how well the GSD model fits into actually observed individual scores. For this we use the $\\chi^2$ goodness-of-fit test.\n",
    "\n",
    "The *chi_squared_test* function calculates the p-value of the $\\chi^2$ test. We need to supply it with a set of information. We need: (i) a vector of ratings for a single test material, (ii) $\\psi$ and $\\rho$ parameters estimated for those ratings and (iii) a value corresponding to the top category of the scale (e.g. for 5-levels ACR this equals 5).\n",
    "\n",
    "The code below shows an exemplary usage of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an exemplary set of individual ratings (we use the 5-levels ACR scale)\n",
    "obs_scores = [1, 2, 3, 4, 4, 4, 3, 4, 3, 5]\n",
    "psi = 3.292  # estimated from obs_scores\n",
    "rho = 0.733  # estimated from obs_scores\n",
    "\n",
    "# Get p-value from the goodness-of-fit test\n",
    "p_val = chi_squared_test(obs_scores, psi, rho, 5)\n",
    "\n",
    "print(\"Chi-squared test p-value: {:.3f}\".format(p_val))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Generalized Score Distribution.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "gsd_chi_squared",
   "language": "python",
   "name": "gsd_chi_squared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
